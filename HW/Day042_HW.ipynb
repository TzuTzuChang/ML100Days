{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "目前你應該已經要很清楚資料集中，資料的型態是什麼樣子囉！包含特徵 (features) 與標籤 (labels)。因此要記得未來不管什麼專案，必須要把資料清理成相同的格式，才能送進模型訓練。\n",
    "今天的作業開始踏入決策樹這個非常重要的模型，請務必確保你理解模型中每個超參數的意思，並試著調整看看，對最終預測結果的影響為何"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "\n",
    "1. 試著調整 DecisionTreeClassifier(...) 中的參數，並觀察是否會改變結果？\n",
    "\n",
    "2. 改用其他資料集 (boston, wine)，並與回歸模型的結果進行比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.試著調整 DecisionTreeClassifier(...) 中的參數，並觀察是否會改變結果？\n",
    "\n",
    "決策樹就是追求不純度相關指標的最優化。\n",
    "\n",
    "> class sklearn.tree.DecisionTreeClassifier (criterion=’gini’, splitter=’best’, max_depth=None, \n",
    "min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n",
    "\n",
    "**重要參數**\n",
    "\n",
    "- criterion\n",
    "\t\n",
    "    對分類樹來說，找出最佳節點及分枝的指標稱為『不純度』，不純度越低，決策樹對訓練集得的擬合越好。\n",
    "\t決策樹就是追求不純度相關指標的最優化。\n",
    "    \n",
    "\t樹中的每個節點都會有一個不純度。\n",
    "    \n",
    "\t子節點的不純度定低於父節點。\n",
    "    \n",
    "\t葉子節點的不純度是最低的。\n",
    "    \n",
    "\t- entropy(信息熵)\n",
    "\t\t\n",
    "        信息熵對不純度叫基尼係數強(對不純度懲罰最強)。\n",
    "\t\t\n",
    "        容易過擬合。\n",
    "        \n",
    "\t- gini(基尼係數)\n",
    "\t\t\n",
    "        適用於數據維度大，噪音很大的數據。\n",
    "\n",
    "*實際使用中兩者效果基本一樣。\n",
    "\n",
    "**隨機性參數**\n",
    "\n",
    "- random_state\n",
    "\t\n",
    "    設置分枝中隨機模式的參數，預設為None。\n",
    "    \n",
    "\t使用同一批訓練集(Xtrain)進行訓練，得到的準確度仍有差異，原因決策樹本身就具有隨機性。\n",
    "    \n",
    "\t輸入任意整數，會針對特定模型選定的feature，長出同一顆樹讓模型穩定下來(固定隨機性)。\n",
    "    \n",
    "\t調參優化時通常會固定random_state。\n",
    "\n",
    "- splitter\n",
    "\t\n",
    "    用作控制決策樹的隨機性。\n",
    "\t\n",
    "    - best\n",
    "\t\t\n",
    "        決策樹雖然本身就具有隨機性，但會優先選擇更重要的特徵進行分枝。\n",
    "        \n",
    "\t- random\n",
    "\t\t\n",
    "        決策樹在分枝時會更為隨機。\n",
    "\t\t\n",
    "        樹深度更深。\n",
    "\t\t\n",
    "        對訓練集的擬合會降低(防止過擬合的一種方式)。\n",
    "\n",
    "**剪枝參數**\n",
    "\n",
    "決策樹會生長到衡量不純度最優，或者沒有更多特徵可用為止。\n",
    "\n",
    "這樣決策樹往往會過擬合。\n",
    "\n",
    "會在訓練集上表現很好，測試集卻很糟糕。\n",
    "\n",
    "為了決策樹有更好的泛化性，須對決策樹進行剪枝。\n",
    "\n",
    "剪枝策略對決策樹的影響巨大，正確的剪枝策略是優化決策樹算法的核心。\n",
    "\n",
    "- max_depth\n",
    "\t\n",
    "    限制樹的最大深度，超過設定深度的樹枝全部剪掉。\n",
    "\t\n",
    "    用的最廣泛的剪枝參數。\n",
    "\t\n",
    "    在高維度、低樣本量時非常有效。\n",
    "\t\n",
    "    在集成算法中也非常實用。\n",
    "\t\n",
    "    建議從= 3開始嘗試。\n",
    "\n",
    "- min_samples_leaf\n",
    "\t\n",
    "    一個節點在分枝後每個子節點都必須包含至少min_samples_leaf個訓練樣本；\n",
    "\t\n",
    "    分枝會朝著滿足每個子節點都包含min_samples_leaf個樣本的方向去發展。\n",
    "\t\n",
    "    一般搭配max_depth使用，建議從= 5開始使用。\n",
    "\t\n",
    "    如果葉節點中含有的樣本量變化很大，建議輸入浮點樹作為樣本量的百分比使用。\n",
    "\n",
    "- min_samples_split\n",
    "\t\n",
    "    一個中間節點必須包含至少min_samples_split個訓練樣本，這個節點在允許被分枝，否則分枝不會發生。\n",
    "\n",
    "- max_features\n",
    "\t\n",
    "    限制分枝時考慮的特徵個數，超過限制個數的特徵都會被捨棄。\n",
    "\t\n",
    "    和max_depth異曲同工。\n",
    "\t\n",
    "    在不知道決策樹中的各個特徵的重要性情況下，設定這個參數會導致學習不足。\n",
    "\n",
    "- min_impurity_decrease\n",
    "\t\n",
    "    限制信息增益的大小。\n",
    "\t\n",
    "    信息增益小於設定數值的分枝不會發生。\n",
    "    \n",
    "其餘參數請參考\n",
    "https://zhuanlan.zhihu.com/p/39780305"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.改用其他資料集 (boston, wine)，並與回歸模型的結果進行比較\n",
    "\n",
    "參考Day_038_HW\n",
    "\n",
    "> 一般線性回歸-Mean squared error: 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics\n",
    "# 如果是分類問題，請使用 DecisionTreeClassifier，若為回歸問題，請使用 DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DecisionTreeRegressor()\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.25, random_state=4)\n",
    "\n",
    "# 建立模型\n",
    "dtr = DecisionTreeClassifier()\n",
    "\n",
    "# 將訓練資料丟進去模型訓練\n",
    "dtr.fit(x_train, y_train)\n",
    "\n",
    "# 將測試資料丟進模型得到預測結果\n",
    "y_pred = dtr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy:  0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Acuuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#???MSE<1, 合理嗎？\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "#print('Mean squared error:%2f'\n",
    "#     % mean_squared_error(y_test, y_pred))\n",
    "#Mean squared error:0.222222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
     ]
    }
   ],
   "source": [
    "print(wine.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance:  [0.01364138 0.03076567 0.         0.         0.         0.\n",
      " 0.12455196 0.         0.         0.39953541 0.         0.04285558\n",
      " 0.38865   ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature importance: \", dtr.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "參考Day_040_HW\n",
    "\n",
    "> 一般線性回歸-Mean squared error:25.419587\n",
    "\n",
    "> Lasso回歸-Mean squared error:28.950512\n",
    "\n",
    "> Ridgec回歸-Mean squared error:25.743684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DecisionTreeRegressor()\n",
    "# 讀取資料集\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2, random_state=1)\n",
    "\n",
    "# 建立模型\n",
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "# 訓練模型\n",
    "dtr.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = dtr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error:16.473824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print('Mean squared error:%2f'\n",
    "     % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance:  [2.33348449e-02 1.31195879e-04 8.75407883e-03 2.81303344e-03\n",
      " 3.01981113e-02 2.54939815e-01 1.17507051e-02 8.14804473e-02\n",
      " 1.17640364e-03 2.47822144e-02 2.09263383e-02 5.59063924e-03\n",
      " 5.34122173e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature importance: \", dtr.feature_importances_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
