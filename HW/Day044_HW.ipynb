{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "確保你了解隨機森林模型中每個超參數的意義，並觀察調整超參數對結果的影響"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "\n",
    "1. 試著調整 RandomForestClassifier(...) 中的參數，並觀察是否會改變結果？\n",
    "2. 改用其他資料集 (boston, wine)，並與回歸模型與決策樹的結果進行比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.試著調整 RandomForestClassifier(...) 中的參數，並觀察是否會改變結果？\n",
    "\n",
    "  \n",
    "> RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, \n",
    "max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, \n",
    "class_weight=None)\n",
    "\n",
    "其大部分的參數幾乎與決策樹一致。\n",
    "\n",
    "**重要參數**\n",
    "\n",
    "其是使用bagging的集成算法都會有的重要參數。\n",
    "\n",
    "- n_estimators\n",
    "    \n",
    "    森林中樹木的數量(base estimator的數量)。\n",
    "    \n",
    "    通常此值越大，模型效果往往越好。\n",
    "    \n",
    "    任何模型都有決策邊界，其值達到一定程度時，精確性往往不在上升或開始波動。\n",
    "    \n",
    "    其值越大，計算量與內存消耗量越大，訓練時間越長。\n",
    "    \n",
    "    渴望在訓練難度和模型效果之間取得平衡。\n",
    "\n",
    "- random_state\n",
    "    \n",
    "    隨機森林中的random_state，其控制的是森林生成的模式(生成一片固定的森林)，而非讓一個森林之中只有一棵樹固定生成的每棵樹的random_state；但是森林中樹跟樹之間還是不一樣，不像決策樹的random_state都是生成同一棵樹。\n",
    "\n",
    "- bootstrap\n",
    "    \n",
    "    bagging是通過樣本抽取後有放回的隨機抽樣技術來型成不同的訓練集。\n",
    "    \n",
    "    bootstrap就是用來控制抽樣技術的參數。\n",
    "    \n",
    "    參數默認為True：有放回的隨機採樣技術。\n",
    "    \n",
    "    通常這個參數不會被我們設置為False。\n",
    "    \n",
    "    由於是有放回的抽樣技術，樣本可能在新的自助集中出現多次，其他樣本可能被忽略。\n",
    "    \n",
    "    自助集大約平均會包含63%的原始數據。\n",
    "    \n",
    "    表示會有37%的數據集被浪費掉，沒有參與建模，其被稱為袋外數據(out of bag data;oob)。\n",
    "    \n",
    "    亦可以被用來作為集成算法的測試集使用。\n",
    "    \n",
    "    亦指在使用隨機森林時，我們可以不劃分測試集和訓練集，只需要用袋外數據來測試模型。\n",
    "    \n",
    "    當原始樣本數及n_estimators不夠大的時候，也很可能沒有數據落在袋外，便無法使用oob數據來測試模型。\n",
    "\n",
    "- oob_score\n",
    "    \n",
    "    若希望用袋外數據測試，實例化時將oob_score設為True。\n",
    "        \n",
    "        RF_classifier = RandomForestClassifier(n_estimators=25, random_state=2, oob_score=True)\n",
    "    \n",
    "    為True時無須再劃分訓練集及測試集。\n",
    "    \n",
    "其餘參數請參考\n",
    "http://www.taroballz.com/2019/05/25/ML_RandomForest_Classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.改用其他資料集 (boston, wine)，並與回歸模型與決策樹的結果進行比較\n",
    "\n",
    "參考Day_040_HW/Day_042_HW\n",
    "\n",
    "> 一般線性回歸-Mean squared error:25.419587\n",
    "\n",
    "> Lasso回歸-Mean squared error:28.950512\n",
    "\n",
    "> Ridgec回歸-Mean squared error:25.743684\n",
    "\n",
    "> DecisionTreeRegressor-Mean squared error:28.950512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料集\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.25, random_state=4)\n",
    "\n",
    "#boston為回歸問題, 須將RandomForestClassifier()改為RandomForestRegressor()\n",
    "# 建立模型 (使用 20 顆樹，每棵樹的最大深度為 4)\n",
    "rfr = RandomForestRegressor(n_estimators=20, max_depth=4)\n",
    "\n",
    "# 訓練模型\n",
    "rfr.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = rfr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.793132951004119\n"
     ]
    }
   ],
   "source": [
    "#準確度計算\n",
    "#acc = metrics.accuracy_score(y_test, y_pred)\n",
    "#print(\"Accuracy: \", acc)\n",
    "##ValueError: continuous is not supported\n",
    "\n",
    "acc = rfr.score(x_test, y_test)\n",
    "print(\"Accuracy: \", acc)\n",
    "##Accuracy:  0.813687399658405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance:  [0.05394426 0.00062869 0.00230565 0.00191397 0.01316012 0.52461315\n",
      " 0.00102931 0.0502181  0.0012122  0.01051316 0.01523787 0.00301732\n",
      " 0.32220621]\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature importance: \", rfr.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error:20.755700\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print('Mean squared error:%2f'\n",
    "     % mean_squared_error(y_test, y_pred))\n",
    "##Mean squared error:20.148271"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
